# From Myth to Machine: The History of Artificial Intelligence

Artificial Intelligence (also known as AI), is a field of computer science that aims to create machines and systems that can perform tasks generally executed through intelligence-related reasoning (both convergent (logical) and divergent (creative) thinking); such as: perception, decision making and natural language processing.

Since the dawn of humankind, people from every land of the world have always wondered on artificial beings who can think and act by themselves. The narration process of said scenarios was expressed through the developement of myths (term that comes from the Greek word "μῦθος" (mûthos), which means "story," "tale," or "narrative”). 

For example, in ancient Egypt, there was a legend about the god Thoth, who was the inventor of writing, mathematics, and magic. He created a living statue of a baboon that could speak and solve problems. He also made a mechanical ibis bird that could fly and report what it saw. Another one, instead, tell the story of the ancient Indian sage Vishwakarma, who was the architect of the gods. He built a flying chariot for the sun god Surya, and also created mechanical animals and birds that could move and act like real ones. He also made a golden lotus that could bloom and close according to the sun’s movement. 

Besides this broad and long-lasting desire of artificial innovation for the aid of human’s life, it is important to note that the modern concept of AI emerged in the mid-20th century, when a group of researchers (such as: Alan Turing, John von Neumann, Claude Shannon etc…) developed the foundations of computation, logic, and information theory. The term “Artificial Intelligence” was coined by John McCarthy in 1956 during a conference on how to develop machines that can handle automated reasoning processes by themselves.

From this time on, AI had some lows and highs in regards to the attention at its developement: initially, during the 1950s and 1960s, there went the creations of the first automated AI programs (such as the Logic Theorist or the General Problem Solver). After that, it went into a black period for many reasons, concerning: unrealistic expectations, ethical problems and technical limitations; lowering the fundings’ sector. This phase (occurred around the 1970s) is referred as the first “AI Winter”. However, with the developement of fuzzy mathematics by the azeri mathmatician Lotfi Aliasker Zadeh, along with the first schemes of neural networks, genetic algorithms and machine learning, AI went into a phase of revival.

However, that won’t last long. During the 1980s and 1990s, AI suffered of the second “AI Winter”, mainly because of the limitations of expert systems that were not scalable to a larger extent. This is going to change, again, when during the 21st century AI will go through very important breakthroughs once again, being able to: image recognition, self-driving vehicles, game playing, machine translations and so forth.

The current state of AI is often described as weak AI, meaning it can perform tasks that otherwise would need human intelligence for, however, it cannot exhibit human-like understanding or creativity with the same precision and intuition.

Once again, this is likely going to change in the future, since scientists foresee the further developement of AGI (Artificial General Intelligence), which is the ability to perform intellectual tasks just as a human. Other researchers, instead, speculate on something referred as ASI (Artificial SuperIntelligence), which is the condition where machines will surpass by a huge extent human intelligence in all aspects.
